---
title: 'XBOC: Explainable Bag-Of-Concepts'
tags:
  - Python
  - natural language processing
  - document classification
  - document embeddings
  - explainable AI
  - topic modelling
authors:
  - name: Kristiyan Sakalyan
    equal-contrib: true
    affiliation: 1
  - name: Gerhard Johann Hagerer
    equal-contrib: true 
    affiliation: 1
  - name: Ahmed Mosharafa
    equal-contrib: true 
    affiliation: 1
affiliations:
 - name: Technical University of Munich (TUM), Germany
   index: 1
date: 08 April 2024
bibliography: paper.bib
---

+-----------------------+--------------+------------+---------------+--------------+
| Model                 | Accuracy (%) | Recall (%) | Precision (%) | F1-Score (%) |
|                       |              |            |               |              |
+:=====================:+:============:+:==========:+:=============:+:============:+
| Logistic Regression   | 95.3         | 95.21      | 95.23         | 95.21        |
+-----------------------+--------------+------------+---------------+--------------+
| Random Forest         | 95.5         | 95.31      | 95.37         | 95.32        |
+-----------------------+--------------+------------+---------------+--------------+
**Table 1:** Performance metrics for Logistic Regression and Random Forest models using explainable document embeddings. Results reflect the models' ability to accurately classify documents in the test set. \label{table:results}

# Summary
This paper introduces a novel approach to generating explainable document embeddings. It leverages enhanced Bag-of-Concepts[@BoC] methodology with a focus on transparency, allowing for clear insight into machine learning decision-making processes. This model not only maintains high accuracy and efficiency but also improves interpretability, making it suitable for applications requiring high levels of trust and accountability, such as legal or medical document analysis.

# Statement of Need
Traditional document embedding techniques often yield opaque outputs, limiting their usability in fields requiring transparency. This work addresses the need for explainable AI in natural language processing by providing a framework that enhances the interpretability of document embeddings without compromising performance. Our approach positions itself crucially among existing works like doc2vec[@Doc2Vec], which focus more on performance than on explainability.

**Problems with existing approaches:** Current embedding methods such as BERT[@BERT], doc2vec[@Doc2Vec], and traditional NLP techniques like bag-of-words and tf-idf often lack explainable features, making it difficult to understand how document contents influence classification outcomes. The features generated by these methods do not clarify the contents of the documents. Existing solutions such as word2vec[@Word2Vec] and its derivatives like Bag-of-Concepts[@BoC], unsupervised aspect detection, and graph-based clustering, provide more explainable features but often require manual labeling of topics or concepts.

**Our solutions:** We propose a hybrid model that incorporates automatically labeled interpretable features similarly to BERTopic[@BERTopic] and the efficiency of the Bag-of-Concepts[@BoC] approach, suitable for processing long texts without the need for GPUs. The model offers a balanced solution that enhances explainability in document embeddings while maintaining performance, making it suitable for advanced NLP applications requiring both transparency and efficiency.

# Features and Functionality

Our enhanced implementation of the Bag-of-Concepts[@BoC] framework introduces several key features that significantly extend its functionality and user flexibility, compared to the original implementation. 
These advancements are designed to provide a more robust, versatile, and user-friendly experience. 
Below, we delineate the novel features integrated into our module:

**Clustering Methods Selection:** Users have the flexibility to choose from various clustering algorithms, enabling the selection of the most appropriate method based on the specific characteristics of their dataset.
    
**Document Encoding Post-Training:** Our system allows for the encoding of new documents after the model has already been trained, facilitating the dynamic application of the model to new data without the need for retraining.
    
**Model Persistence:** Users can save and subsequently load the trained model, enhancing the efficiency of repeated analyses and the deployment of models in different contexts.
    
**Retrieval of Top N Words:** The module provides functionality to retrieve the top N words associated with each concept, offering insights into the semantic composition of the concept space.
    
**Concept Space Labeling:** Through integration with a pre-defined or custom large language model pipeline, our framework supports the labeling of the concept space, thereby enriching the interpretability of the model's output.
    
**Optimality Scores Calculation:** It incorporates the calculation of Bayesian Information Criterion (BIC), Akaike Information Criterion (AIC), and other scores to aid in determining the most optimal number of concepts for a given dataset.
    
**Interpretable Concept Space:** The interpretable nature of the concept space enables the explanation of decisions taken by machine learning models applied on the Concept Frequency-Inverse Document Frequency (CF-IDF) values, thereby enhancing the transparency of model predictions.
    
**SHAP Values Visualization:** Our implementation is compatible with the visualization of SHapley Additive exPlanations (SHAP) values[@SHAP].

These features collectively aim to augment the analytical capabilities of researchers and practitioners, offering a comprehensive toolset for sfemantic analysis and model interpretation within the Bag-of-Concepts[@BoC] framework.

# Example Use Cases
Our model could be particularly useful in fields where decision-making transparency is critical. In legal environments, it can help legal professionals understand how AI tools analyze and classify documents, ensuring that automated recommendations adhere to legal standards. In medical research, the model could aid in literature screening by explaining why certain studies are relevant, which is crucial for systematic reviews and meta-analyses. Financial compliance officers can leverage our explainable embeddings to scrutinize transactions and communications, pinpointing reasons behind flagged activities to improve compliance monitoring and reduce false positives.

Additionally, businesses could apply our model to analyze customer feedback, categorizing and understanding underlying sentiments transparently to enhance customer service. Policymakers and public administrators can benefit from our approach when interpreting public policy documents, ensuring policy decisions are informed and transparently derived. In academia, the model can support the categorization and peer review of research papers and grant applications, making funding and publication decisions clear and justifiable.

# Example usage and experimental evaluation
The code repository of this paper contains an example for classification of news reports of the BBC[@BBC-News] News dataset. The newspaper articles are assigned to a given category by a classifier. Experiments show the accuracies shown in \autoref{table:results} 1. The notebooks also depict the SHAP[@SHAP] values and explain which topics are related to which newspaper categories. This can be used to understand how topics are inter-related amongst each other.

# Key References
- BOC
- BERTopic

# Acknowledgements
Special thanks to the contributors and colleagues from the Technical University of Munich (TUM) who provided insights and expertise that greatly assisted the research.
